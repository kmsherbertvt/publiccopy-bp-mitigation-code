{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'rename'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_111616/3463615002.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mdf_layerwise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_layerwise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_df_layerwise\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m \u001b[0mdf_layerwise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;31m#df_layerwise.drop('point', inplace=True, axis=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'rename'"
     ]
    }
   ],
   "source": [
    "# Barren Plateaus Data Analysis\n",
    "\n",
    "## Load Data\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rc(\"savefig\", dpi=500)\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import glob\n",
    "import ast\n",
    "\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "plt.rcParams['savefig.dpi'] = 150\n",
    "\n",
    "data_path = './bps/maxcut/'\n",
    "sns.set_palette('viridis', n_colors=100)\n",
    "\n",
    "def array_interpret(s: str) -> np.array:\n",
    "    try:\n",
    "        return np.array(ast.literal_eval(f'[{s}]'))[0]\n",
    "    except:\n",
    "        return np.array([])\n",
    "\n",
    "def interpret_complex(s:str) -> float:\n",
    "    if '+' in s:\n",
    "        re, im = s.split('+')\n",
    "        re = float(re)\n",
    "        im = im.rstrip(\" \")[0:-2]\n",
    "        im = float(im)\n",
    "        if abs(im) > 0.0:\n",
    "            raise ValueError(\"Non-zero imaginary component!!!\")\n",
    "    else:\n",
    "        re = float(s)\n",
    "    return re\n",
    "\n",
    "def gate_counts(l):\n",
    "    n = len(l[0])\n",
    "    return Counter(map(lambda s: n - s.count(\"I\"), l))\n",
    "\n",
    "\"\"\" `df_layerwise` contains information collected during the inner loop (VQE\n",
    "subroutine) of ADAPT-VQE.\n",
    "\"\"\"\n",
    "\n",
    "df_layerwise = None\n",
    "\n",
    "for file_path in glob.glob(data_path+'**/layer_*.csv', recursive=True):\n",
    "    _, _, _, _, _, ham, num_qubits, pool, seed, layer = file_path.split(\"/\")\n",
    "    num_qubits = int(num_qubits)\n",
    "    seed = int(seed)\n",
    "    layer = int(layer.split(\"_\")[1].split(\".\")[0])\n",
    "    \n",
    "    _df_layerwise = pd.read_csv(\n",
    "            file_path,\n",
    "            delimiter=';',\n",
    "            converters={\n",
    "                'iter': int,\n",
    "                ' cost': float,\n",
    "                ' point': array_interpret,\n",
    "                ' grad': array_interpret}\n",
    "        )\n",
    "    _df_layerwise['num_qubits'] = num_qubits\n",
    "    _df_layerwise['seed'] = seed\n",
    "    _df_layerwise['layer'] = layer\n",
    "    \n",
    "    if df_layerwise is None:\n",
    "        df_layerwise = _df_layerwise\n",
    "    else:\n",
    "        df_layerwise = pd.concat([df_layerwise, _df_layerwise])\n",
    "df_layerwise.rename(columns=lambda x: x.lstrip(' '), inplace=True)\n",
    "#df_layerwise.drop('point', inplace=True, axis=1)\n",
    "\n",
    "groups = []\n",
    "\n",
    "for name, group in df_layerwise.groupby(['num_qubits', 'seed']):\n",
    "    num_qubits, seed = name\n",
    "    num_layers = max(group['layer'])\n",
    "    group['final_layer'] = group['layer'].map(lambda l: l == num_layers)\n",
    "    groups.append(group)\n",
    "df_layerwise = pd.concat(groups).reset_index()\n",
    "\n",
    "\"\"\" `df_result` contains information about the final convergence properties\n",
    "of ADAPT-VQE. This includes information about the success of the ADAPT-VQE\n",
    "algorithm.\n",
    "\n",
    "`df_adapt` contains information collected during the outer loop of\n",
    "ADAPT-VQE, for example gradients computed and operators selected, etc.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "df_result = None\n",
    "df_adapt = None\n",
    "\n",
    "for exact_file_path in glob.glob(data_path+'**/exact_result.csv', recursive=True):\n",
    "    _, _, _, _, _, ham, num_qubits, pool, seed, _ = exact_file_path.split(\"/\")\n",
    "    adapt_file_path = exact_file_path.split('exact')[0] + 'adapt_history.csv'\n",
    "    num_qubits = int(num_qubits)\n",
    "    seed = int(seed)\n",
    "    \n",
    "    _df_result_1 = pd.read_csv(exact_file_path, delimiter=';', converters={\n",
    "        ' min_bs': str, ' min_val': interpret_complex})\n",
    "    try:\n",
    "        _df_result_2 = pd.read_csv(adapt_file_path, delimiter=';')\n",
    "    except:\n",
    "        continue\n",
    "    _df_result_1.rename(columns=lambda x: x.lstrip(' '), inplace=True)\n",
    "    _df_result_2.rename(columns=lambda x: x.lstrip(' '), inplace=True)\n",
    "    \n",
    "    _df_result_1['num_qubits'] = num_qubits\n",
    "    _df_result_1['seed'] = seed\n",
    "    _df_result_1['min_bs'] = _df_result_1['min_bs'].map(lambda bs: bs.rjust(num_qubits, '0'))\n",
    "    \n",
    "    #####################\n",
    "    # Process Pauli lists\n",
    "    if len(_df_result_1) > 1:\n",
    "        raise ValueError(\"Hmm...\")\n",
    "    _df_result_1['pauli_list'] = [None] * len(_df_result_1)\n",
    "    _df_result_1['pauli_list'] = [list(map(lambda s: s.lstrip(), list(_df_result_2['paulis'])))]\n",
    "    _df_result_1['two_local_op_count'] = _df_result_1['pauli_list'].map(lambda g: gate_counts(g)[2])\n",
    "    #####################\n",
    "    \n",
    "    _df_adapt = pd.concat([pd.concat([_df_result_1]*len(_df_result_2), ignore_index=True), _df_result_2], axis=1, )\n",
    "    \n",
    "    if df_adapt is None:\n",
    "        df_adapt = _df_adapt\n",
    "    else:\n",
    "        df_adapt = pd.concat([df_adapt, _df_adapt])\n",
    "    \n",
    "df_adapt.reset_index(inplace=True)\n",
    "df_adapt.drop(columns='index', inplace=True)\n",
    "df_adapt = df_adapt[df_adapt['opt_numevals'] != ' nothing']\n",
    "df_adapt['grads'] = df_adapt['grads'].map(array_interpret)\n",
    "df_adapt['opt_pars'] = df_adapt['opt_pars'].map(array_interpret)\n",
    "df_adapt['opt_numevals'] = df_adapt['opt_numevals'].map(int)\n",
    "df_adapt['norm_c'] = df_adapt['energy'] / df_adapt['min_val']\n",
    "\n",
    "\n",
    "for name, group in df_adapt.groupby(['num_qubits', 'seed']):\n",
    "    num_qubits, seed = name\n",
    "    num_layers = max(group['layer'])\n",
    "    group = group[group['layer'] == num_layers]\n",
    "    if df_result is None:\n",
    "        df_result = group\n",
    "    else:\n",
    "        df_result = pd.concat([group, df_result])\n",
    "\n",
    "df_adapt.drop(columns=['pauli_list', 'two_local_op_count'], inplace=True)\n",
    "\n",
    "df_adapt['num_qubits'].value_counts()\n",
    "\n",
    "df_result['num_qubits'].value_counts()\n",
    "\n",
    "df_layerwise['num_qubits'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximation Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot demonstrates the success of the ADAPT-VQE algorithm on the family of max cut Hamiltonians. The y axis is the approximation ratio, which measures performance. It is calculated by taking the value of the cost function determined by the algorithm and dividing by the exact answer. This means that `1.0` is a perfect result.\n",
    "\n",
    "We can see that for all numbers of qubits considered, the approximation ratio goes roughly to `1.0`. This means that on average, the algorithm succeeds with sufficiently many layers of ADAPT.\n",
    "\n",
    "It is natural to ask: How many layers is sufficient to converge, as a function of the number of qubits?\n",
    "\n",
    "(Compare with Fig 5 https://arxiv.org/pdf/2004.04197.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lineplot(data=df_adapt, x='layer', y='norm_c', hue='num_qubits')\n",
    "#ax = sns.scatterplot(data=df_adapt, x='layer', y='norm_c', hue='num_qubits')\n",
    "ax.set_xlabel(\"ADAPT Layer\")\n",
    "ax.set_ylabel(\"<C>/C_min\")\n",
    "ax.set_xticks(np.arange(0, 20+1, step=4))\n",
    "ax.legend(title=\"Num Qubits\", loc='lower right');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers to Convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take the above data and consider only the approximation ratio at final layer of ADAPT, i.e. the actual result. As a function of the number of qubits, the number of layers needed to converge ADAPT fits well to a linear regression. The implication of this result is that for the setup considered, ADAPT requires circuits of depth `O(n)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.regplot(data=df_result, x='num_qubits', y='layer')\n",
    "ax.set_xlabel(\"Num Qubits\")\n",
    "ax.set_ylabel(\"Layers to Convergence\")\n",
    "ax.set_xticks(np.arange(4, 20+1, step=4))\n",
    "ax.set_yticks(np.arange(4, 20+1, step=4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detail of Approximation Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a closer look at the final value of the approximation ratio. Here we consider the final result as a function of the number of qubits. There are two plots on this figure, both representing the same information. The solid line with the shaded region are the mean of the approximation ratio for that number of qubits and the 95% confidence interval for the data, respectively. The dots are the raw data.\n",
    "\n",
    "This plot tells us that on average, the approximation ratio consistently stays above `0.9`. However, there are some exceptions to this statement for larger numbers of qubits. More interestingly, there seem to be three clusters of data. I do not yet have an explanation for this.\n",
    "\n",
    "It is important to note that there are many dots clustered around and approximation ratio of `1.0`. For example, for 20 qubits, 60% of the data are at an approximation ratio of `1.0`.\n",
    "\n",
    "(Compare with Fig 4 https://arxiv.org/pdf/2004.04197.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lineplot(data=df_result, x='num_qubits', y='norm_c')\n",
    "ax = sns.scatterplot(data=df_result, x='num_qubits', y='norm_c')\n",
    "#ax = sns.boxplot(data=df_result, x='num_qubits', y='norm_c', whis=np.inf)\n",
    "ax.set_xlabel(\"Num Qubits\")\n",
    "ax.set_ylabel(\"<C>/C_min\");\n",
    "#ax.set_xticks(np.arange(0, 20+1, step=4));\n",
    "#ax.set_ylim(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Circuit Depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another performance check, we can ask how many 2-local operators are being chosen to reach a given approximation ratio. This is important because the current form of the ansatz does not have the QAOA structure, so there is no direct comparison with the value of `p` in the algorithm. Hence, we count the number of 2-local operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax = sns.lineplot(data=df_result, x='two_local_op_count', y='norm_c', hue='num_qubits')\n",
    "ax = sns.scatterplot(data=df_result, x='two_local_op_count', y='norm_c', hue='num_qubits')\n",
    "ax.set_xlabel(\"2-local Operator Count\")\n",
    "ax.set_ylabel(\"<C>/C_min\")\n",
    "ax.set_xticks(np.arange(0, 20+1, step=4))\n",
    "ax.legend(title=\"Num Qubits\", loc='lower left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all following discussion, when we refer to values of the gradient we are referring to the **components** of the gradient, rather than the norm of the gradient vector.\n",
    "\n",
    "Thusfar, the results have been focused on understanding the convergence of the ADAPT for this particular problem. These results are interesting in their own rite (specifically the observation that ADAPT converges in `O(n)` layers), however we are also interested in understanding the effect (or lack thereof) of barren plateaus on the ADAPT algorithm.\n",
    "\n",
    "To this end, we begin by showing the gradients computed **during the inner loop VQE optimization** as a function of each layer of ADAPT. Note that this is **not** displaying the `~<[H,O]>` gradients computed in the **outer loop**.\n",
    "\n",
    "From this plot we can see that the gradients generally decrease in magnitude as the number of layers increases, however, we are interested in the asymptotic behavior of the gradients and their variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"This plot shows components of the gradient calculated during\n",
    "the inner loop (VQE subroutine) of ADAPT-VQE, as a function of the\n",
    "number of layers and number of qubits.\n",
    "\n",
    "For each number of qubits, the gradient converges at some number of\n",
    "layers. Visually, the number of layers required to converge seems to\n",
    "increase linearly with the number of qubits. This is significant\n",
    "because it is numerical evidence that (for the max cut problem and\n",
    "2-local pool), ADAPT-VQE requires `O(n)` layers to converge.\n",
    "\"\"\"\n",
    "_df_layerwise = df_layerwise.explode('grad')\n",
    "_df_layerwise.grad = _df_layerwise.grad.astype(float)\n",
    "_df_layerwise.grad = _df_layerwise.grad.map(abs)\n",
    "ax = sns.lineplot(data=_df_layerwise, x='layer', y='grad', hue='num_qubits')\n",
    "ax.set_xlabel(\"ADAPT Layer\")\n",
    "ax.set_ylabel(\"|Cost Function Gradient|\")\n",
    "ax.set_yscale('log')\n",
    "ax.set_xticks(np.arange(0, 20+1, step=4))\n",
    "ax.legend(title=\"Num Qubits\", loc='upper right');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CDF Gradient Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand the distributions of the gradients, we will focus on the gradients computed (again, during the **inner VQE loop**) at the final layer of ADAPT.\n",
    "\n",
    "We plot the cumulative distribution function (CDF) of the gradients computed for varying numbers of qubits. The cumulative distribution function (each line on the plot) represents the proportion (the y axis) of the data that fall below each given value on the x axis. Hence, the lines are monotonically increasing, and all have a limit `1.0` as the x axis increases.\n",
    "\n",
    "The trend in the CDFs as a function of the number of qubits is that they tend to shift to the left as the number of qubits increases. From the last figure, this result is expected. However, we want to know _how quickly_ this happens.\n",
    "\n",
    "We can see that although the CDFs shift to the left as a function of the number of qubits, they tend to bunch up closer together as the number of qubits increases. **(I am still thinking about the conclusions of this plot)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.ecdfplot(data=_df_layerwise[_df_layerwise['final_layer'] == True], x='grad', hue='num_qubits')\n",
    "ax.set_xscale('log')\n",
    "#ax.set_yscale('log')\n",
    "ax.set_xlabel(\"Gradient\")\n",
    "ax.set_title(\"CDF of Gradients at Last Layer of ADAPT\");\n",
    "#ax.set_xlim(1e-19, 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do the same, fixing the number of qubits and analyzing different layers of ADAPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.ecdfplot(data=_df_layerwise[_df_layerwise['num_qubits'] == 20], x='grad', hue='layer')\n",
    "ax.set_xscale('log')\n",
    "#ax.set_yscale('log')\n",
    "ax.set_xlabel(\"Gradient\")\n",
    "ax.set_title(\"CDF of Gradients for 20 qubits\");\n",
    "#ax.set_xlim(1e-19, 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance of Gradient Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the most common method of investigating barren plateaus is by analyzing the variance of the gradient as a function of the number of qubits. If the variance decays as a function of the number of qubits slower than exponentially, then we can say that barren plateaus are mitigated by ADAPT.\n",
    "\n",
    "It is currently not clear whether or not this is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lineplot(\n",
    "    data=_df_layerwise.groupby(by=['num_qubits']).aggregate('var').reset_index(),\n",
    "    x='num_qubits',\n",
    "    y='grad'\n",
    ")\n",
    "ax.set_ylabel(\"Var(grad)\")\n",
    "ax.set_xlabel(\"Num Qubits\");\n",
    "#ax.set_yscale(\"log\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outer Loop Gradient Convergence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_tmp = df_adapt[df_adapt['num_qubits'] == 20]\n",
    "\n",
    "_df_tmp = _df_tmp.explode('grads')\n",
    "_df_tmp.grads = _df_tmp.grads.astype(float)\n",
    "_df_tmp.grads = _df_tmp.grads.map(abs)\n",
    "\n",
    "ax = sns.ecdfplot(data=_df_tmp, x='grads', hue='layer')\n",
    "#ax.legend(loc='lower left')\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel(\"Gradient\")\n",
    "ax.set_ylim(5.5*1e-1, 1.0)\n",
    "ax.set_xlim(1e-61, 1e2)\n",
    "ax.axvline(1e-6, ls='--') # Gradient threshold\n",
    "ax.set_title(\"CDF of *Outer Loop* Gradients for 20 Qubits\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Function Calls Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_data_optimizer = []\n",
    "\n",
    "for name, group in df_layerwise.groupby(by=['num_qubits', 'seed', 'layer']):\n",
    "    num_qubits, seed, layer = name\n",
    "    num_iters = len(group)\n",
    "    _data_optimizer.append({\n",
    "        'num_qubits': num_qubits,\n",
    "        'seed': seed,\n",
    "        'layer': layer,\n",
    "        'num_iters': num_iters\n",
    "    })\n",
    "\n",
    "_df_optimizer = pd.DataFrame(_data_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(_df_optimizer, col=\"num_qubits\", col_wrap=3, height=2)\n",
    "\n",
    "g.map(sns.histplot, 'layer', 'num_iters', cbar=True)\n",
    "g.set_ylabels('Num Iterations')\n",
    "g.set_xlabels('Layer')\n",
    "\n",
    "for ax in g.axes:\n",
    "    num_qubits = ax.title.get_text().split(' ')[-1]\n",
    "    ax.set_title(f'Num Qubits: {num_qubits}')\n",
    "    ax.set_xticks(range(0, 21, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's next?\n",
    "\n",
    "* Perform random point sampling so I can see how deep the anatze are when BPs emerge\n",
    "* Include additional pools in simulations\n",
    "* Max-2-Sat problem?\n",
    "* Directly compute expressibility of ansatze?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
